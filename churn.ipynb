{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce2d247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "!pip install pandas numpy matplotlib seaborn scikit-learn xgboost diffprivlib cryptography\n",
    "from diffprivlib.models import LogisticRegression as DPLogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from google.colab import files\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, classification_report, roc_curve\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier, Perceptron, RidgeClassifier, PassiveAggressiveClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, ExtraTreesClassifier, BaggingClassifier, HistGradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from xgboost import XGBClassifier\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "# Suppress ConvergenceWarnings\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "from google.colab import files\n",
    "uploaded = files.upload()\n",
    "\n",
    "df = pd.read_csv('Churn_Modelling.csv', delimiter=',')\n",
    "df.shape\n",
    "\n",
    "# Check columns list and missing values\n",
    "df.isnull().sum()\n",
    "\n",
    "# Get unique count for each variable\n",
    "df.nunique()\n",
    "\n",
    "# Drop the columns as explained above\n",
    "df = df.drop([\"RowNumber\", \"CustomerId\", \"Surname\"], axis = 1)\n",
    "\n",
    "# Review the top rows of what is left of the data frame\n",
    "df.head()\n",
    "\n",
    "\n",
    "# Check variable data types\n",
    "df.dtypes\n",
    "\n",
    "labels = 'Exited', 'Retained'\n",
    "sizes = [df.Exited[df['Exited']==1].count(), df.Exited[df['Exited']==0].count()]\n",
    "explode = (0, 0.1)\n",
    "fig1, ax1 = plt.subplots(figsize=(10, 8))\n",
    "ax1.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%',\n",
    "        shadow=True, startangle=90)\n",
    "ax1.axis('equal')\n",
    "plt.title(\"Proportion of customer churned and retained\", size = 20)\n",
    "plt.show()\n",
    "\n",
    "# Categorical variables analysis\n",
    "fig, axarr = plt.subplots(2, 2, figsize=(20, 12))\n",
    "sns.countplot(x='Geography', hue = 'Exited',data = df, ax=axarr[0][0])\n",
    "sns.countplot(x='Gender', hue = 'Exited',data = df, ax=axarr[0][1])\n",
    "sns.countplot(x='HasCrCard', hue = 'Exited',data = df, ax=axarr[1][0])\n",
    "sns.countplot(x='IsActiveMember', hue = 'Exited',data = df, ax=axarr[1][1])\n",
    "\n",
    "# Continuous variables analysis\n",
    "fig, axarr = plt.subplots(3, 2, figsize=(20, 12))\n",
    "sns.boxplot(y='CreditScore',x = 'Exited', hue = 'Exited',data = df, ax=axarr[0][0])\n",
    "sns.boxplot(y='Age',x = 'Exited', hue = 'Exited',data = df , ax=axarr[0][1])\n",
    "sns.boxplot(y='Tenure',x = 'Exited', hue = 'Exited',data = df, ax=axarr[1][0])\n",
    "sns.boxplot(y='Balance',x = 'Exited', hue = 'Exited',data = df, ax=axarr[1][1])\n",
    "sns.boxplot(y='NumOfProducts',x = 'Exited', hue = 'Exited',data = df, ax=axarr[2][0])\n",
    "sns.boxplot(y='EstimatedSalary',x = 'Exited', hue = 'Exited',data = df, ax=axarr[2][1])\n",
    "\n",
    "\n",
    "# Split Train, test data\n",
    "df_train = df.sample(frac=0.8,random_state=200)\n",
    "df_test = df.drop(df_train.index)\n",
    "print(len(df_train))\n",
    "print(len(df_test))\n",
    "\n",
    "df_train['BalanceSalaryRatio'] = df_train.Balance/df_train.EstimatedSalary\n",
    "sns.boxplot(y='BalanceSalaryRatio',x = 'Exited', hue = 'Exited',data = df_train)\n",
    "plt.ylim(-1, 5)\n",
    "\n",
    "\n",
    "# Introduce new features\n",
    "df_train['TenureByAge'] = df_train.Tenure/(df_train.Age)\n",
    "sns.boxplot(y='TenureByAge',x = 'Exited', hue = 'Exited',data = df_train)\n",
    "plt.ylim(-1, 1)\n",
    "plt.show()\n",
    "\n",
    "df_train['CreditScoreGivenAge'] = df_train.CreditScore/(df_train.Age)\n",
    "\n",
    "\n",
    "# Resulting Data Frame\n",
    "df_train.head()\n",
    "\n",
    "\n",
    "df_train = df.sample(frac=0.8, random_state=200)\n",
    "df_test = df.drop(df_train.index)\n",
    "\n",
    "\n",
    "# Feature engineering\n",
    "df_train['BalanceSalaryRatio'] = df_train.Balance / df_train.EstimatedSalary\n",
    "df_train['TenureByAge'] = df_train.Tenure / df_train.Age\n",
    "df_train['CreditScoreGivenAge'] = df_train.CreditScore / df_train.Age\n",
    "\n",
    "\n",
    "# Arrange columns by data type\n",
    "continuous_vars = ['CreditScore', 'Age', 'Tenure', 'Balance', 'NumOfProducts', 'EstimatedSalary']  # Removed the engineered features\n",
    "cat_vars = ['HasCrCard', 'IsActiveMember', 'Geography', 'Gender']\n",
    "\n",
    "# Select the desired columns from the original DataFrame 'df'\n",
    "df_train = df[['Exited'] + continuous_vars + cat_vars].copy()\n",
    "\n",
    "# Feature engineering - Now recreate these columns after selecting from 'df'\n",
    "df_train.loc[:, 'BalanceSalaryRatio'] = df_train.Balance / df_train.EstimatedSalary\n",
    "df_train.loc[:, 'TenureByAge'] = df_train.Tenure / df_train.Age\n",
    "df_train.loc[:, 'CreditScoreGivenAge'] = df_train.CreditScore / df_train.Age\n",
    "\n",
    "df_train.head()\n",
    "\n",
    "\n",
    "'''For the one hot variables, we change 0 to -1 so that the models can capture a negative relation\n",
    "where the attribute in inapplicable instead of 0'''\n",
    "df_train.loc[df_train.HasCrCard == 0, 'HasCrCard'] = -1\n",
    "df_train.loc[df_train.IsActiveMember == 0, 'IsActiveMember'] = -1\n",
    "df_train.head()\n",
    "\n",
    "\n",
    "# One hot encode the categorical variables\n",
    "lst = ['Geography', 'Gender']\n",
    "remove = list()\n",
    "for i in lst:\n",
    "    # Use 'object' instead of 'np.str' or 'np.object'\n",
    "    if (df_train[i].dtype == object):\n",
    "        for j in df_train[i].unique():\n",
    "            df_train[i+'_'+j] = np.where(df_train[i] == j,1,-1)\n",
    "        remove.append(i)\n",
    "df_train = df_train.drop(remove, axis=1)\n",
    "df_train.head()\n",
    "\n",
    "\n",
    "# MinMax scaling the continuous variables\n",
    "minVec = df_train[continuous_vars].min().copy()\n",
    "maxVec = df_train[continuous_vars].max().copy()\n",
    "df_train[continuous_vars] = (df_train[continuous_vars]-minVec)/(maxVec-minVec)\n",
    "df_train.head()\n",
    "\n",
    "\n",
    "# data prep pipeline for test data\n",
    "def DfPrepPipeline(df_predict,df_train_Cols,minVec,maxVec):\n",
    "    # Add new features\n",
    "    df_predict['BalanceSalaryRatio'] = df_predict.Balance/df_predict.EstimatedSalary\n",
    "    df_predict['TenureByAge'] = df_predict.Tenure/(df_predict.Age - 18)\n",
    "    df_predict['CreditScoreGivenAge'] = df_predict.CreditScore/(df_predict.Age - 18)\n",
    "\n",
    "# Reorder the columns\n",
    "continuous_vars = ['CreditScore', 'Age', 'Tenure', 'Balance', 'NumOfProducts', 'EstimatedSalary', 'BalanceSalaryRatio',\n",
    "                   'TenureByAge', 'CreditScoreGivenAge']\n",
    "cat_vars = ['HasCrCard', 'IsActiveMember', \"Geography\", \"Gender\"]\n",
    "\n",
    "\n",
    "# Calculate new features for df_test\n",
    "df_test['BalanceSalaryRatio'] = df_test.Balance / df_test.EstimatedSalary\n",
    "df_test['TenureByAge'] = df_test.Tenure / (df_test.Age - 18)\n",
    "df_test['CreditScoreGivenAge'] = df_test.CreditScore / (df_test.Age - 18)\n",
    "\n",
    "# Create the test DataFrame\n",
    "df_predict = df_test[['Exited'] + continuous_vars + cat_vars]\n",
    "\n",
    "df_predict.head()\n",
    "\n",
    "\n",
    "df_predict.loc[df_predict.HasCrCard == 0, 'HasCrCard'] = -1\n",
    "df_predict.loc[df_predict.IsActiveMember == 0, 'IsActiveMember'] = -1\n",
    "\n",
    "\n",
    "# One hot encode the categorical variables\n",
    "lst = [\"Gender\",\"Geography\"]  # Removed 'Geography' as it's already processed\n",
    "remove = list()\n",
    "\n",
    "\n",
    "# One hot encode the categorical variables\n",
    "lst = [\"Gender\",\"Geography\"]  # Removed 'Geography' as it's already processed\n",
    "remove = list()\n",
    "\n",
    "for i in lst:\n",
    "    # Check if column exists before proceeding\n",
    "    if i in df_train.columns and df_train[i].dtype == object:\n",
    "        for j in df_train[i].unique():\n",
    "            df_train[i + '_' + j] = np.where(df_train[i] == j, 1, -1)\n",
    "        remove.append(i)\n",
    "\n",
    "# Remove columns outside the loop to avoid issues\n",
    "df_train = df_train.drop(remove, axis=1)\n",
    "\n",
    "# MinMax scaling\n",
    "minVec = df_train[continuous_vars].min().copy()\n",
    "maxVec = df_train[continuous_vars].max().copy()\n",
    "df_train[continuous_vars] = (df_train[continuous_vars] - minVec) / (maxVec - minVec)\n",
    "print(df_train.head())\n",
    "\n",
    "\n",
    "# Ensure that all one hot encoded variables that appear in the train data appear in the test data\n",
    "L = list(set(df_train.columns) - set(df_predict.columns))\n",
    "for l in L:\n",
    "    df_predict[str(l)] = -1\n",
    "\n",
    "# MinMax scaling the continuous variables based on min and max from the train data\n",
    "df_predict[continuous_vars] = (df_predict[continuous_vars] - minVec) / (maxVec - minVec)\n",
    "\n",
    "# Ensure that the variables are ordered in the same way as was ordered in the train set\n",
    "df_predict = df_predict[df_train.columns] # Use 'columns' instead of 'column'\n",
    "\n",
    "\n",
    "# Support functions\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from scipy.stats import uniform\n",
    "\n",
    "# Fit models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Scoring functions\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "\n",
    "def evaluate_model(model, X_train, y_train, X_val, y_val):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_val)\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    auc = roc_auc_score(y_val, y_pred)\n",
    "    print(f\"{model.__class__.__name__}\")\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"AUC Score: {auc}\")\n",
    "    print()\n",
    "\n",
    "# Split the training data for validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(df_train.loc[:, df_train.columns != 'Exited'], df_train.Exited, test_size = 0.2, random_state = 0)\n",
    "\n",
    "\n",
    "# Implementing Differential Privacy in Logistic Regression\n",
    "dp_lr = DPLogisticRegression(epsilon=1.0)\n",
    "dp_lr.fit(X_train, y_train)\n",
    "dp_y_pred = dp_lr.predict(X_val)\n",
    "\n",
    "\n",
    "# Model Evaluation\n",
    "dp_acc = accuracy_score(y_val, dp_y_pred)\n",
    "dp_auc = roc_auc_score(y_val, dp_y_pred)\n",
    "print(\"Differentially Private Logistic Regression\")\n",
    "print(f\"Accuracy: {dp_acc}\")\n",
    "print(f\"AUC Score: {dp_auc}\")\n",
    "print()\n",
    "\n",
    "\n",
    "#Function for Differential Privacy\n",
    "def dp_noise_addition(data, epsilon=1.0):\n",
    "    noise = np.random.laplace(loc=0.0, scale=1/epsilon, size=data.shape)\n",
    "    return data + noise\n",
    "\n",
    "\n",
    "\n",
    "# Adding noise to sensitive columns\n",
    "df_train_noised = df_train.copy()\n",
    "sensitive_columns = ['CreditScore', 'Balance', 'EstimatedSalary']\n",
    "df_train_noised[sensitive_columns] = dp_noise_addition(df_train[sensitive_columns], epsilon=1.0)\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(df_train_noised.loc[:, df_train_noised.columns != 'Exited'])\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "\n",
    "# Train the model\n",
    "lr = LogisticRegression(max_iter=1000)  # Increase max_iter if needed\n",
    "lr.fit(X_train_scaled, df_train_noised['Exited'])\n",
    "lr_y_pred = lr.predict(X_val_scaled)\n",
    "\n",
    "\n",
    "# Model Evaluation\n",
    "acc = accuracy_score(y_val, lr_y_pred)\n",
    "auc = roc_auc_score(y_val, lr_y_pred)\n",
    "print(\"Standard Logistic Regression with Noised Data\")\n",
    "print(f\"Accuracy: {acc}\")\n",
    "print(f\"AUC Score: {auc}\")\n",
    "print()\n",
    "\n",
    "\n",
    "# Train and evaluate different models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, ExtraTreesClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier # Import KNeighborsClassifier\n",
    "\n",
    "models = [\n",
    "    LogisticRegression(),\n",
    "    SVC(probability=True),\n",
    "    RandomForestClassifier(),\n",
    "    XGBClassifier(),\n",
    "    KNeighborsClassifier(), # Now you can use it\n",
    "    GradientBoostingClassifier(),\n",
    "    AdaBoostClassifier(),\n",
    "    DecisionTreeClassifier(),\n",
    "    GaussianNB(),\n",
    "    ExtraTreesClassifier()\n",
    "]\n",
    "\n",
    "\n",
    "for model in models:\n",
    "    evaluate_model(model, X_train, y_train, X_val, y_val)\n",
    "\n",
    "# Function for Differential Privacy\n",
    "def dp_noise_addition(data, epsilon=1.0):\n",
    "    noise = np.random.laplace(scale=1/epsilon, size=data.shape)\n",
    "    return data + noise\n",
    "\n",
    "# Sample usage of Differential Privacy\n",
    "df_train_private = df_train.copy()\n",
    "df_train_private.loc[:, df_train_private.columns != 'Exited'] = dp_noise_addition(df_train_private.loc[:, df_train_private.columns != 'Exited'])\n",
    "\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier, HistGradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.linear_model import SGDClassifier, Perceptron, RidgeClassifier, PassiveAggressiveClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier, AdaBoostClassifier, ExtraTreesClassifier, RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from diffprivlib.models import LogisticRegression as DPLogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.exceptions import ConvergenceWarning # Import ConvergenceWarning\n",
    "import warnings\n",
    "\n",
    "# Suppress ConvergenceWarnings\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "\n",
    "# Initialize models\n",
    "models = {\n",
    "    'Differentially Private Logistic Regression': DPLogisticRegression(epsilon=1.0, data_norm=1.0),\n",
    "    'Logistic Regression with Noised Data': LogisticRegression(max_iter=1000),\n",
    "    'K-Nearest Neighbors': KNeighborsClassifier(),\n",
    "    'Decision Tree': DecisionTreeClassifier(),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(),\n",
    "    'AdaBoost': AdaBoostClassifier(),\n",
    "    'Naive Bayes': GaussianNB(),\n",
    "    'Extra Trees': ExtraTreesClassifier(),\n",
    "    'Random Forest': RandomForestClassifier(),\n",
    "    'SVC': SVC(probability=True),\n",
    "    'XGBoost': XGBClassifier(),\n",
    "    'Bagging Classifier': BaggingClassifier(),\n",
    "    'HistGradientBoostingClassifier': HistGradientBoostingClassifier(),\n",
    "    'SGDClassifier': SGDClassifier(),\n",
    "    'Perceptron': Perceptron(),\n",
    "    'RidgeClassifier': RidgeClassifier(),\n",
    "    'PassiveAggressiveClassifier': PassiveAggressiveClassifier()\n",
    "}\n",
    "\n",
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "\n",
    "# Train and evaluate each model\n",
    "for name, model in models.items():\n",
    "    if 'Differentially Private' in name:\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        y_pred = model.predict(X_val_scaled)\n",
    "    elif 'Noised Data' in name:\n",
    "        model.fit(df_train_noised.loc[:, df_train_noised.columns != 'Exited'], df_train_noised['Exited'])\n",
    "        y_pred = model.predict(X_val_scaled)\n",
    "    else:\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        y_pred = model.predict(X_val_scaled)\n",
    "\n",
    "    acc = accuracy_score(y_val, y_pred)\n",
    "    auc = roc_auc_score(y_val, y_pred)\n",
    "    print(f\"{name} - Accuracy: {acc}, AUC: {auc}\")\n",
    "\n",
    "\n",
    "# Adding Voting Classifier for ensemble\n",
    "voting_clf = VotingClassifier(estimators=[\n",
    "    ('lr', LogisticRegression(max_iter=1000)),\n",
    "    ('rf', RandomForestClassifier()),\n",
    "    ('gnb', GaussianNB())],\n",
    "    voting='soft')\n",
    "voting_clf.fit(X_train_scaled, y_train)\n",
    "y_pred = voting_clf.predict(X_val_scaled)\n",
    "acc = accuracy_score(y_val, y_pred)\n",
    "auc = roc_auc_score(y_val, y_pred)\n",
    "print(f\"Voting Classifier - Accuracy: {acc}, AUC: {auc}\")\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "import warnings\n",
    "from cryptography.fernet import Fernet\n",
    "import hashlib\n",
    "import requests\n",
    "\n",
    "# Suppress ConvergenceWarnings\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "# Encryption setup\n",
    "def generate_key():\n",
    "    return Fernet.generate_key()\n",
    "\n",
    "def encrypt_data(data, key):\n",
    "    fernet = Fernet(key)\n",
    "    encrypted = fernet.encrypt(data.encode())\n",
    "    return encrypted\n",
    "\n",
    "def decrypt_data(encrypted_data, key):\n",
    "    fernet = Fernet(key)\n",
    "    decrypted = fernet.decrypt(encrypted_data).decode()\n",
    "    return decrypted\n",
    "\n",
    "# Placeholder for loading and decrypting data\n",
    "def load_data(file_path, key):\n",
    "    df = pd.read_csv(file_path)\n",
    "    # Decrypt sensitive columns if necessary\n",
    "    return df\n",
    "\n",
    "# Define roles and permissions\n",
    "roles_permissions = {\n",
    "    'Admin': ['view_data', 'modify_data', 'encrypt_data', 'decrypt_data'],\n",
    "    'User': ['view_data']\n",
    "}\n",
    "\n",
    "# Log list to store access attempts\n",
    "access_log = []\n",
    "\n",
    "# Function to check if a user has the required permission and log the attempt\n",
    "def check_access(user, permission):\n",
    "    role = user_roles.get(user)\n",
    "    authorized = role and permission in roles_permissions.get(role, [])\n",
    "    access_log.append({'user': user, 'permission': permission, 'authorized': authorized})\n",
    "    return authorized\n",
    "\n",
    "# Function to review access logs and identify unauthorized access attempts\n",
    "def review_access_logs():\n",
    "    print(\"\\nReviewing Access Logs:\")\n",
    "    unauthorized_attempts = [log for log in access_log if not log['authorized']]\n",
    "    if unauthorized_attempts:\n",
    "        print(\"Unauthorized Access Attempts Found:\")\n",
    "        for attempt in unauthorized_attempts:\n",
    "            print(f\"User: {attempt['user']}, Permission: {attempt['permission']}, Authorized: {attempt['authorized']}\")\n",
    "    else:\n",
    "        print(\"No unauthorized access attempts found.\")\n",
    "\n",
    "# Analyze data integrity\n",
    "def analyze_data_integrity(df, user):\n",
    "    if not check_access(user, 'view_data'):\n",
    "        print(f\"Access denied for user {user} to perform data integrity analysis.\")\n",
    "        return\n",
    "\n",
    "    print(\"Data Integrity Analysis:\")\n",
    "    # Example: Checking for duplicates\n",
    "    if df.duplicated().sum() > 0:\n",
    "        print(\"Warning: Duplicate records found in the data.\")\n",
    "    else:\n",
    "        print(\"No duplicate records found.\")\n",
    "\n",
    "    # Example: Hashing and comparing\n",
    "    original_hash = hashlib.sha256(pd.util.hash_pandas_object(df).values).hexdigest()\n",
    "    print(f\"Data Hash: {original_hash}\")\n",
    "\n",
    "# Security risk analysis\n",
    "def analyze_security_risks(df, user):\n",
    "    if not check_access(user, 'view_data'):\n",
    "        print(f\"Access denied for user {user} to perform security risk analysis.\")\n",
    "        return\n",
    "\n",
    "    print(\"Security Risk Analysis:\")\n",
    "\n",
    "    # Check for anomalous behavior or malicious patterns\n",
    "    # Example: Simple anomaly detection based on statistical measures\n",
    "    print(\"Checking for anomalous behavior or malicious patterns:\")\n",
    "    if (df.describe().loc['std'] > 100).any():  # This is a simplified example\n",
    "        print(\"Warning: High variability detected in some columns, which may indicate anomalies.\")\n",
    "\n",
    "    # Malware detection: Checking for known malware signatures\n",
    "    print(\"Malware Detection:\")\n",
    "    known_malware_signatures = [\"ransomware_signature\", \"trojan_signature\", \"spyware_signature\", \"adware_signature\"]\n",
    "    detected_malware = [sig for sig in known_malware_signatures if check_for_malware_signature(df, sig)]\n",
    "    if detected_malware:\n",
    "        print(f\"Detected Malware Signatures: {detected_malware}\")\n",
    "    else:\n",
    "        print(\"No known malware signatures detected.\")\n",
    "\n",
    "    # Example: Check for access control issues\n",
    "    print(\"Access Control Analysis:\")\n",
    "    # Placeholder: You might want to log access details here\n",
    "    print(\"Access controls are in place to limit access to sensitive data.\")\n",
    "\n",
    "# Example function to check for malware signature (simplified)\n",
    "def check_for_malware_signature(df, signature):\n",
    "    # Simplified example: Check if the signature is in any of the string columns\n",
    "    return any(df[col].astype(str).str.contains(signature).any() for col in df.select_dtypes(include=[object]).columns)\n",
    "\n",
    "# Mitigation strategies\n",
    "def apply_mitigation(df_train, key, user):\n",
    "    if not check_access(user, 'encrypt_data'):\n",
    "        print(f\"Access denied for user {user} to apply mitigation strategies.\")\n",
    "        return\n",
    "\n",
    "    print(\"Applying Mitigation Strategies:\")\n",
    "    # Encrypt sensitive columns\n",
    "    key = generate_key()\n",
    "    print(f\"Encryption Key: {key.decode()}\")\n",
    "    # Save the key securely; here it is just printed for demonstration\n",
    "\n",
    "    # Placeholder: Encrypting data (example)\n",
    "    df_train_noised = df_train.copy()\n",
    "    df_train_noised['EncryptedBalance'] = df_train_noised['Balance'].apply(lambda x: encrypt_data(str(x), key))\n",
    "\n",
    "    # Implementing access controls\n",
    "    print(\"1. Implementing Role-Based Access Controls.\")\n",
    "    print(\"2. Encrypting sensitive data.\")\n",
    "    print(\"3. Conducting regular security audits.\")\n",
    "    print(\"4. Adversarial training for model robustness.\")\n",
    "\n",
    "# Define test users and actions\n",
    "test_users = [\n",
    "    {'user': 'alice', 'role': 'Admin'},  # Admin user\n",
    "    {'user': 'bob', 'role': 'User'},     # Regular user\n",
    "    {'user': 'eve', 'role': 'User'},     # Regular user trying unauthorized actions\n",
    "]\n",
    "\n",
    "# Assign roles to test users\n",
    "user_roles = {user['user']: user['role'] for user in test_users}\n",
    "\n",
    "# Function to perform test actions\n",
    "def perform_test_actions():\n",
    "    print(\"\\nPerforming Test Actions:\")\n",
    "\n",
    "    # Admin user performs all actions\n",
    "    current_user = 'alice'\n",
    "    print(f\"\\nTesting actions for {current_user} (Admin):\")\n",
    "    analyze_data_integrity(df, current_user)\n",
    "    analyze_security_risks(df, current_user)\n",
    "    apply_mitigation(df, key, current_user)\n",
    "\n",
    "    # Regular user performs allowed and unauthorized actions\n",
    "    current_user = 'bob'\n",
    "    print(f\"\\nTesting actions for {current_user} (User):\")\n",
    "    analyze_data_integrity(df, current_user)\n",
    "    analyze_security_risks(df, current_user)\n",
    "    apply_mitigation(df, key, current_user)  # Should be unauthorized\n",
    "\n",
    "    # Another regular user performs unauthorized actions\n",
    "    current_user = 'eve'\n",
    "    print(f\"\\nTesting actions for {current_user} (User):\")\n",
    "    analyze_data_integrity(df, current_user)\n",
    "    apply_mitigation(df, key, current_user)  # Should be unauthorized\n",
    "\n",
    "from google.colab import files\n",
    "uploaded = files.upload()\n",
    "\n",
    "\n",
    "# Example usage\n",
    "key = generate_key()\n",
    "df = load_data('Churn_Modelling.csv', key)  # Adjust as needed for encrypted data\n",
    "\n",
    "# Execute test actions\n",
    "perform_test_actions()\n",
    "\n",
    "# Review and report unauthorized access attempts\n",
    "review_access_logs()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
